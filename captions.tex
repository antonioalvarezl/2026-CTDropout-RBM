% LaTeX figure captions for the paper.
% Copy each into the appropriate \caption{...} in the manuscript.

% ─── Experiment 1 — Forward pass (§5.1) ───────────────────────────────────

% fig1_trajectories
From left to right: training data on two concentric circles;
full NODE trajectories with constant parameters;
rNODE trajectories (averaged over $K$ realisations) with constant parameters;
full NODE trajectories with time-dependent parameters $W(t),b(t)$;
rNODE trajectories with time-dependent parameters.
Endpoints are coloured by class label.

% fig2_convergence
Log-log plot of $\max_{t}\mathbb{E}_\omega[\|x_t-\hat x_t\|^2]$ versus
the switching step~$h$ for time-dependent (left) and constant (right)
parameters. Red crosses: reference slope~$1$; black dots: numerical errors
with 95\% confidence intervals; blue dashed: fitted regression line.
The observed slopes confirm the $O(h)$ rate predicted by Theorem~1.

% fig3_decision_{const,tdep}
Decision boundaries for constant-parameter (top) and time-dependent
(bottom) models. From left to right: full NODE, rNODE with $h=\Delta t$,
$h=2\Delta t$, $h=3\Delta t$. Larger~$h$ progressively smooths the
boundary, illustrating the regularisation effect of random batching.

% fig4_benchmark_{const,tdep}
Wall-clock forward-pass time versus dataset size for the full NODE (red)
and rNODE (blue) with $r=8$. Shaded bands: $\pm 1$ std.

% fig5_cost_vs_error
Computational cost versus trajectory error for five batch sampling schemes
at $h\in\{0.001,0.01,0.1\}$. Each marker is one $(h,\text{scheme})$ pair.

% fig6_scheme_convergence
Convergence rate comparison across four batch schemes.
All exhibit $O(h)$ rate; the leading constant depends on~$\pi_{\min}$.
Dashed: reference slope~$1$.

% fig7_variance_by_scheme
Empirical variance $\mathrm{Var}_\omega[\max_t\|x_t-\hat x_t\|^2]$
versus~$h$ for four schemes. Smaller~$\pi_{\min}$ yields higher variance,
validating the $\Lambda_t$ dependence in Table~1.

% fig8_pareto
Error versus batch size~$r$ at fixed $h$. Dashed red: Pareto front.
Diminishing returns appear beyond $r\approx 8$.

% fig9_optimal_h
Left: empirical error $\mathcal{E}(h)$ with fitted power law.
Right: theoretical $h^*$ (\S4.2) versus empirical $h^*$. Agreement along
the diagonal confirms the closed-form cost--accuracy trade-off.

% fig10_error_constant_vs_pimin
Error constant $S = \mathcal{E}/h$ versus the minimum inclusion
probability~$\pi_{\min}$ for all canonical schemes. Smaller~$\pi_{\min}$
yields a larger error constant, confirming the dependence predicted by
Theorem~1. Dashed line: power-law fit.

% fig11_speedup
Wall-clock speedup of rNODE over the full NODE versus $\pi = r/p$.
Measured on forward-pass integration; dashed line marks the break-even
speedup of~$1\times$.

% ─── Experiment 2 — Flow matching (§5.2) ──────────────────────────────────

% fig_flow1_data
Initial density $\rho_0$ (blue, Gaussian at $(-1,-1)$) and target
$\rho_1$ (orange, mixture of three Gaussians).

% fig_flow2_density
Full-model density evolution at $t\in\{0,0.5,1\}$ via KDE on mesh
particles integrated with the midpoint rule ($\Delta t=0.01$).

% fig_flow2_trajectories
Same density panel with 30 flow lines overlaid, showing transport from
the initial Gaussian to the three target modes.

% fig_flow3_comparison
Left: final-time density $\rho_T$ (full model). Right: rNODE density
averaged over $K=20$ realisations with 3 batches. The rNODE reproduces the
tri-modal target with slight broadening.

% fig_flow4_convergence
$\mathbb{E}[\|\rho_T-\hat\rho_T\|_{L^1}]$ versus~$h$. Red crosses:
reference slope~$1/2$; black dots: numerical errors (95\% CI); blue
dashed: fitted slope. Confirms the $O(\sqrt{h})$ rate of Corollary~3.

% fig_flow5_scheme_conv
$L^1$ convergence for four batch counts. All exhibit $O(\sqrt{h})$; the
leading constant grows with the number of batches.

% fig_flow6_variance
Variance of $L^1$ error versus~$h$ for four batch counts.

% fig_flow7_benchmark
Forward-pass time (ms) for full model and rNODE with 2--8 batches.

% ─── Experiment 3a — Training, all parameters (§5.3) ─────────────────────

% ex3a_node_dynamics
Trajectories of the full time-dependent NODE with ELU activation
trained on the two-circles dataset ($p=24$, $T=2$, 1000 epochs).
Lines show the flow $t\mapsto x_t$; squares mark the final positions.

% ex3a_rnode_dynamics
Ensemble-averaged rNODE trajectories ($K=10$ realisations, 3 batches,
fixed schedule per realisation). The averaged flow is smoother than the
individual realisations but retains the same classification accuracy.

% ex3a_decision_boundaries
Decision boundaries after training. From left to right: full NODE, rNODE
with $h=\Delta t$, $h=2\Delta t$, $h=3\Delta t$.
Larger~$h$ smooths the boundary (dropout regularisation).
Test points overlaid.

% ex3a_benchmark
Train (blue) and test (orange) accuracy for the full NODE and rNODE
ensembles with 2, 3, 4, and 6 disjoint batches.

% ─── Experiment 3b — Training, fixed inner weights (§5.3) ────────────────

% ex3b_scaling
Scaling analysis with fixed inner weights.
Left: train and test accuracy versus hidden dimension~$p$.
Centre: training time versus~$p$.
Right: number of trainable parameters versus~$p$.
Accuracy saturates around $p\approx 96$--$192$; training time grows
linearly with~$p$.

% ex3b_decision_boundaries
Decision boundaries for the fixed-inner-weights model ($p=24$). From left
to right: full NODE, rNODE with $h=\Delta t$, $h=2\Delta t$, $h=3\Delta t$.
Test points overlaid.

% ex3b_benchmark
Train (blue) and test (orange) accuracy for the fixed-inner NODE and rNODE
ensembles with 2, 3, 4, and 6 batches. Comparable accuracy despite
training only the outer layer.
